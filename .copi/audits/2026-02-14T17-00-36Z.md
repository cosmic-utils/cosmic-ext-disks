# Audit Report: cosmic-ext-disks Architecture & Code Quality

**Date:** 2026-02-14T17:00:36Z  
**Branch:** `feature/storage-service`  
**Commit:** `e38fe8bde2c3e047288d77398cfd820509abd47e`  
**Audit Scope:** All crates in workspace after storage-service refactor  
**Repository:** cosmic-utils/cosmic-ext-disks

---

## Executive Summary

This audit identifies **11 critical architectural flaws** and **15 medium-priority issues** that compromise separation of concerns, maintainability, and reliability following the storage-service refactor. The most urgent issues are:

1. **CRITICAL: Architectural Layer Violation** — `storage-service` directly imports and calls `disks-dbus` internal types (30+ callsites), violating the intended architecture where `storage-service` should only depend on `storage-models`

2. **CRITICAL: Optional Connection Anti-Pattern** — `VolumeNode` and `VolumeModel` use `connection: Option<Connection>` leading to 20+ `unwrap()` calls that can panic at runtime

3. **CRITICAL: Synchronous Runtime Creation in Async Context** — `UiDrive::clone()` creates new `tokio::Runtime` and blocks synchronously, a severe anti-pattern causing potential deadlocks

4. **HIGH: Excessive Unwrap() Usage** — 100+ `unwrap()` / `expect()` calls without proper error propagation, many in production code paths

5. **HIGH: Missing Parent Path Population** — Comment in [disks-dbus/src/disks/volume.rs:651](../disks-dbus/src/disks/volume.rs#L651) says "TODO: Populate when building hierarchy" but is never implemented, breaking UI tree construction in some cases

---

## Surface Map

### Entry Points

**storage-service** (`storage-service/src/main.rs`)
- D-Bus service on system bus: `org.cosmic.ext.StorageService`
- Handlers: Disks, Partitions, Filesystems, LUKS, LVM, BTRFS, Image
- Root check: requires `euid == 0`
- Socket activation support enabled

**disks-ui** (`disks-ui/src/main.rs`)
- COSMIC application using libcosmic/iced
- Client-side D-Bus consumers for storage-service
- No direct udisks2 access (uses storage-service as intermediary)

### Crate Structure

```
storage-service/     → D-Bus service (root-privileged operations)
├─ Handlers for each domain (disks, partitions, filesystems, etc.)
├─ Polkit authorization checks
└─ DEPENDS ON: disks-dbus (PROBLEM), storage-models

storage-models/      → Canonical domain types (DiskInfo, VolumeInfo, etc.)
└─ Serde-serializable, shared across all components

disks-dbus/          → UDisks2 abstraction layer
├─ DriveModel, VolumeNode, VolumeModel
├─ Operations: mount/unmount/format/etc.
└─ DEPENDS ON: udisks2, storage-models

disks-btrfs/         → BTRFS-specific operations
└─ SubvolumeManager, usage calculation

disks-ui/            → Frontend application
├─ D-Bus clients (DisksClient, FilesystemsClient, etc.)
├─ UI models (UiDrive, UiVolume) wrapping storage-models types
└─ COSMIC/libcosmic views and state management
```

### Public API Surfaces

**storage-service D-Bus interfaces** (all at `/org/cosmic/ext/StorageService/<handler>`):
- `Disks`: list_disks, list_volumes, get_disk_info, get_volume_info, eject, power_off, run_smart_test
- `Partitions`: list_partitions, create_partition_table, create_partition, delete_partition, resize_partition
- `Filesystems`: list_filesystems, format, mount, unmount, check, set_label, get_usage
- `LUKS`: list_luks, unlock, lock, change_passphrase, get_encryption_options, set_encryption_options
- `LVM`: list_volume_groups, list_logical_volumes, list_physical_volumes, create_volume_group
- `BTRFS`: get_subvolumes, create_subvolume, delete_subvolume, create_snapshot, get_usage
- `Image`: export_image, restore_image

**disks-ui client modules** (`client/{disks,partitions,filesystems,luks,lvm,btrfs,image}.rs`):
- Async wrappers for D-Bus interfaces
- JSON deserialization into storage-models types
- Error conversion (zbus → ClientError)

---

## Findings (Prioritized Backlog)

### GAP-001: storage-service Performing Operations Directly Instead of Delegating

**ID:** GAP-001  
**Type:** Architecture / Separation of Concerns  
**Severity:** CRITICAL  
**Impact:** storage-service contains business logic that should be in lower layers; untestable; violates intended architecture

**Evidence:**

**storage-service imports udisks2 directly and creates D-Bus proxies:**

[storage-service/src/partitions.rs:9](../storage-service/src/partitions.rs#L9):
```rust
use udisks2::{partition::PartitionProxy, partitiontable::PartitionTableProxy, block::BlockProxy};
```

[storage-service/src/filesystems.rs:10](../storage-service/src/filesystems.rs#L10):
```rust
use udisks2::{block::BlockProxy, filesystem::FilesystemProxy};
```

[storage-service/src/luks.rs:8](../storage-service/src/luks.rs#L8):
```rust
use udisks2::{block::BlockProxy, encrypted::EncryptedProxy};
```

**storage-service creates UDisks2 proxies and calls D-Bus methods directly (30+ callsites):**

[storage-service/src/partitions.rs:296-315](../storage-service/src/partitions.rs#L296-L315):
```rust
let table_proxy = PartitionTableProxy::builder(connection)
    .path(&block_path)
    .map_err(|e| { /* ... */ })?
    .build()
    .await
    .map_err(|e| { /* ... */ })?;

let partition_path = table_proxy
    .create_partition(offset, size, &type_id, "", options)
    .await
    .map_err(|e| { /* ... */ })?;
```

[storage-service/src/filesystems.rs:239-271](../storage-service/src/filesystems.rs#L239-L271):
```rust
let block_proxy = BlockProxy::builder(connection)
    .path(&block_path)
    /* ... */
    .build()
    .await
    /* ... */

block_proxy.format(&fs_type, format_opts).await
    .map_err(|e| { /* ... */ })?;
```

**storage-service does direct file I/O for image operations:**

[storage-service/src/image.rs:203](../storage-service/src/image.rs#L203):
```rust
let source_file = std::fs::File::open(&input_path)
```

[storage-service/src/image.rs:134](../storage-service/src/image.rs#L134):
```rust
let dest_file = std::fs::OpenOptions::new()
    .write(true)
    .create(true)
    .open(&dest_path)
```

**Root Cause:**

The architecture should have 3 clear layers:
1. **storage-service** (D-Bus service): Polkit auth, JSON serialization, signal emission, orchestration only
2. **disks-dbus** (library): All UDisks2 D-Bus operations
3. **storage-sys** (library, NEW): Direct system calls (file I/O, process management, etc.)

Instead, storage-service is:
- Creating UDisks2 proxies directly
- Calling D-Bus methods (format, mount, create_partition, etc.)
- Opening file descriptors and doing file I/O
- Mixing orchestration with implementation

This makes storage-service:
- Impossible to test without full UDisks2 daemon
- Difficult to maintain (business logic mixed with D-Bus boilerplate)
- Cannot be replaced with alternative implementations

**Suggested Fix:**

**Phase 1: Move UDisks2 operations to disks-dbus**

Add operation functions to disks-dbus public API:
```rust
// In disks-dbus/src/lib.rs or operations.rs
pub async fn create_partition(
    disk: &str,
    offset: u64,
    size: u64,
    type_id: &str,
) -> Result<String, DiskError> {
    // All the proxy building and D-Bus calls here
}

pub async fn format_filesystem(
    device: &str,
    fs_type: &str,
    label: &str,
    options: FormatOptions,
) -> Result<(), DiskError> {
    // Format implementation here
}

pub async fn mount_filesystem(
    device: &str,
    mount_point: &str,
    options: MountOptions,
) -> Result<String, DiskError> {
    // Mount implementation here
}
```

**Phase 2: Create storage-sys crate for system operations**

```rust
// storage-sys/src/lib.rs
pub mod image;
pub mod process;

// storage-sys/src/image.rs
pub fn open_for_backup(device: &str) -> Result<OwnedFd, SysError> {
    // File descriptor operations
}

pub fn copy_with_progress(
    source: OwnedFd,
    dest: &Path,
    progress_callback: impl Fn(u64),
) -> Result<(), SysError> {
    // File copying logic
}
```

**Phase 3: Refactor storage-service to orchestrate only**

```rust
// storage-service/src/partitions.rs (after refactor)
async fn create_partition(
    &self,
    #[zbus(connection)] connection: &Connection,
    #[zbus(signal_context)] signal_ctx: zbus::object_server::SignalEmitter<'_>,
    disk: String,
    offset: u64,
    size: u64,
    type_id: String,
) -> zbus::fdo::Result<String> {
    // 1. Authorization
    check_polkit_auth(connection, "org.cosmic.ext.storage-service.partition-modify")
        .await
        .map_err(|e| zbus::fdo::Error::from(e))?;
    
    // 2. Call disks-dbus (NO D-Bus operations here)
    let device_path = disks_dbus::create_partition(&disk, offset, size, &type_id)
        .await
        .map_err(|e| zbus::fdo::Error::Failed(e.to_string()))?;
    
    // 3. Signal emission
    let _ = Self::partition_created(&signal_ctx, &disk, &device_path, "{}").await;
    
    Ok(device_path)
}
```

**Acceptance Criteria:**
- [ ] storage-service has ZERO `use udisks2::` imports
- [ ] storage-service has ZERO `Proxy::builder()` calls
- [ ] storage-service has ZERO `std::fs::File::open()` or direct file I/O
- [ ] storage-service methods are <20 lines (auth + delegate + signal)
- [ ] All UDisks2 D-Bus operations in disks-dbus
- [ ] All file/process operations in storage-sys (new crate)
- [ ] `cargo check` passes
- [ ] Tests can mock disks-dbus without UDisks2 daemon

---

### GAP-002: Optional Connection Anti-Pattern in VolumeNode/VolumeModel

**ID:** GAP-002  
**Type:** Reliability / Error Handling  
**Severity:** CRITICAL  
**Impact:** Runtime panics when connection is None; unclear ownership semantics; API misuse easy

**Evidence:**

[disks-dbus/src/disks/volume.rs:44](../disks-dbus/src/disks/volume.rs#L44):
```rust
pub struct VolumeNode {
    // ... fields ...
    connection: Option<Connection>,
}
```

[disks-dbus/src/disks/volume.rs:296](../disks-dbus/src/disks/volume.rs#L296):
```rust
let backend = RealDiskBackend::new(self.connection.as_ref().unwrap().clone());
```

Found 20+ `.unwrap()` calls on `self.connection.as_ref()` across:
- [volume.rs](../disks-dbus/src/disks/volume.rs): lines 296, 305, 483, 491, 500, 515, 536, 557
- [volume_model/partition.rs](../disks-dbus/src/disks/volume_model/partition.rs): lines 17, 69, 90
- [volume_model/filesystem.rs](../disks-dbus/src/disks/volume_model/filesystem.rs): lines 12, 30, 46, 68, 90

**Root Cause:**

`VolumeNode` and `VolumeModel` carry an `Option<Connection>` field that:
1. Is always `Some` when constructed from real operations
2. Is set to `None` when testing or when cloning without context
3. **Every operation method** unwraps it, causing panic if None

This pattern suggests the connection should not be optional at all. The `Option` wrapper provides false flexibility and makes the API unsafe.

**Suggested Fix:**

**Option A: Required Connection (Preferred)**
```rust
pub struct VolumeNode {
    // ... fields ...
    connection: Connection,  // Not optional
}

impl VolumeNode {
    pub async fn from_block_object(
        connection: &Connection,  // Borrowed
        // ...
    ) -> Result<Self> {
        // Store clone in struct
        connection: connection.clone()
    }
}
```

**Option B: Arc-Wrapped Connection**
```rust
pub struct VolumeNode {
    connection: Arc<Connection>,  // Shared ownership, never None
}
```

**Option C: Separate Operation Context**
```rust
pub struct VolumeNode {
    // Remove connection field entirely
}

pub struct VolumeOperations {
    connection: Connection,
}

impl VolumeOperations {
    pub async fn mount(&self, volume: &VolumeNode) -> Result<()> {
        // Operations use self.connection
    }
}
```

**Acceptance Criteria:**
- [ ] No `Option<Connection>` fields in VolumeNode or VolumeModel
- [ ] Zero `unwrap()` calls on connection access
- [ ] Tests use mock/test connections, not None
- [ ] All operations have clear connection lifecycle
- [ ] `cargo clippy` passes without warnings

---

### GAP-003: Blocking Runtime Creation in Clone (UiDrive)

**ID:** GAP-003  
**Type:** Performance / Concurrency  
**Severity:** CRITICAL  
**Impact:** Deadlock risk; synchronous blocking in async context; poor performance

**Evidence:**

[disks-ui/src/models/ui_drive.rs:206-208](../disks-ui/src/models/ui_drive.rs#L206-L208):
```rust
impl Clone for UiDrive {
    fn clone(&self) -> Self {
        // Create new client instances (blocking runtime for sync context)
        let rt = tokio::runtime::Runtime::new().expect("Failed to create runtime");
        let client = rt.block_on(DisksClient::new()).expect("Failed to create DisksClient");
        let partitions_client = rt.block_on(PartitionsClient::new()).expect("Failed to create PartitionsClient");
        
        Self {
            disk: self.disk.clone(),
            volumes: self.volumes.clone(),
            partitions: self.partitions.clone(),
            client,
            partitions_client,
        }
    }
}
```

Similar pattern in [disks-ui/src/models/helpers.rs:93-94](../disks-ui/src/models/helpers.rs#L93-L94):
```rust
let rt = tokio::runtime::Runtime::new().ok()?;
let client = rt.block_on(FilesystemsClient::new()).ok()?;
```

**Root Cause:**

`UiDrive` owns `DisksClient` and `PartitionsClient` which are async-constructed. When implementing `Clone`, the author:
1. Created a **new** tokio runtime
2. Used `block_on()` to construct clients synchronously
3. This is called from **synchronous** context (Clone trait)

**Problems:**
- Creating a runtime is expensive (~1ms)
- `block_on()` from an already-running async context can deadlock
- If this clone happens on the main UI thread while async work is pending → freeze/deadlock
- Violates tokio best practices (should have 1 runtime per process)

**Suggested Fix:**

**Option A: Remove Clone, Use Arc**
```rust
#[derive(Debug)]
pub struct UiDrive {
    pub disk: DiskInfo,
    pub volumes: Vec<UiVolume>,
    pub partitions: Vec<PartitionInfo>,
    client: Arc<DisksClient>,
    partitions_client: Arc<PartitionsClient>,
}

// Clone is automatically derived; clients are Arc-cloned (cheap reference count)
```

**Option B: Make Clone Return a Future**
```rust
impl UiDrive {
    pub async fn clone_async(&self) -> Result<Self, ClientError> {
        Ok(Self {
            disk: self.disk.clone(),
            volumes: self.volumes.clone(),
            partitions: self.partitions.clone(),
            client: DisksClient::new().await?,
            partitions_client: PartitionsClient::new().await?,
        })
    }
}
// Remove impl Clone
```

**Option C: Lazily Initialize Clients**
```rust
pub struct UiDrive {
    pub disk: DiskInfo,
    pub volumes: Vec<UiVolume>,
    pub partitions: Vec<PartitionInfo>,
    client: OnceCell<DisksClient>,
    partitions_client: OnceCell<PartitionsClient>,
}

impl UiDrive {
    async fn get_client(&self) -> Result<&DisksClient> {
        self.client.get_or_try_init(|| DisksClient::new()).await
    }
}
```

**Acceptance Criteria:**
- [ ] No `Runtime::new()` calls in disks-ui
- [ ] No `block_on()` calls outside of main.rs or test harnesses
- [ ] Clone is either derived or removed
- [ ] Async context used properly throughout
- [ ] No performance regression in UI responsiveness

---

### GAP-004: Missing Parent Path Population

**ID:** GAP-004  
**Type:** Bug / Missing Logic  
**Severity:** HIGH  
**Impact:** UI hierarchy construction may fail for some volume types; tree relationships incomplete

**Evidence:**

[disks-dbus/src/disks/volume.rs:651](../disks-dbus/src/disks/volume.rs#L651):
```rust
parent_path: None, // TODO: Populate when building hierarchy
```

This appears in the `From<VolumeNode> for VolumeInfo` conversion. The TODO has been in place since initial refactor but never implemented.

**Impact Analysis:**

`VolumeInfo` in storage-models has a `parent_path` field used by UI to build hierarchical trees:
```rust
pub struct VolumeInfo {
    pub parent_path: Option<String>,  // Used for tree building
    // ...
}
```

The UI relies on this in [disks-ui/src/models/helpers.rs](../disks-ui/src/models/helpers.rs) to call `build_volume_tree()`. Without correct parent_path:
- Volumes appear as root-level when they should be nested
- LUKS cleartext devices may not show under their encrypted container
- LVM logical volumes may not nest under physical volumes

**Root Cause:**

During the refactor, conversion from `VolumeNode` → `VolumeInfo` was added, but the recursive parent-child relationship tracking was not implemented. The `VolumeNode` already has `children: Vec<VolumeNode>`, but when flattening to `Vec<VolumeInfo>`, the parent linkage is lost.

**Suggested Fix:**

Implement parent tracking in the flattening function:

```rust
impl From<VolumeNode> for VolumeInfo {
    fn from(node: VolumeNode) -> Self {
        Self {
            kind: node.kind,
            label: node.label,
            size: node.size,
            device_path: node.device_path.clone(),
            // Compute parent_path from context during tree walk
            parent_path: None,  // Caller must set this
            // ...
        }
    }
}

// Update storage-service disks.rs flatten_volumes() to track parent:
fn flatten_volumes(
    node: &disks_dbus::VolumeNode,
    parent_device: Option<String>,
    output: &mut Vec<storage_models::VolumeInfo>,
) {
    let mut vol_info: storage_models::VolumeInfo = node.clone().into();
    
    // SET PARENT PATH
    vol_info.parent_path = parent_device.clone();
    
    // Recurse with THIS device as parent for children
    let current_device = vol_info.device_path.clone();
    for child in &node.children {
        flatten_volumes(child, current_device.clone(), output);
    }
    
    vol_info.children.clear();
    output.push(vol_info);
}
```

**Acceptance Criteria:**
- [ ] All VolumeInfo objects have correct parent_path populated
- [ ] LUKS cleartext devices show parent_path = encrypted container device
- [ ] LVM LVs show parent_path = PV device
- [ ] Nested BTRFS subvolumes have correct parent paths
- [ ] UI tree construction works for all volume types
- [ ] Remove TODO comment

---

### GAP-005: Excessive Unwrap/Expect Usage (100+ instances)

**ID:** GAP-005  
**Type:** Reliability / Error Handling  
**Severity:** HIGH  
**Impact:** Runtime panics on unexpected conditions; poor error messages; production crashes

**Evidence:**

Grep shows 100+ matches for `unwrap()` and `expect()`:
- [storage-models/src/disk.rs](../storage-models/src/disk.rs): Tests use `unwrap()` (acceptable)
- [storage-models/src/common.rs:119](../storage-models/src/common.rs#L119): `let string_value = string_value.unwrap();`
- [storage-models/src/common.rs:122](../storage-models/src/common.rs#L122): `let unit = pretty.split_whitespace().last().unwrap();`
- [storage-service/src/luks.rs:51](../storage-service/src/luks.rs#L51): Fallback with nested `unwrap()`
- [disks-btrfs/src/subvolume.rs:68](../disks-btrfs/src/subvolume.rs#L68): `path.strip_prefix("<FS_TREE>/").unwrap()`
- [disks-dbus/src/disks/ops.rs](../disks-dbus/src/disks/ops.rs): 30+ `lock().unwrap()` on Mutex
- [disks-ui/src/logging.rs](../disks-ui/src/logging.rs): 8x `expect("Invalid log directive: ...")`

**Categories:**

1. **Test-only unwraps** (acceptable): storage-models tests, disks-dbus tests
2. **Mutex lock unwraps** (concerning): `Mutex` poisoning not handled
3. **String parsing unwraps** (bug-prone): `split_whitespace().last().unwrap()` panics on empty strings
4. **Path manipulation unwraps** (bug-prone): `strip_prefix().unwrap()` assumes prefix always exists
5. **Static initialization expects** (acceptable if justified): logging setup, static data

**Suggested Fix:**

**For production code:**
```rust
// BAD
let unit = pretty.split_whitespace().last().unwrap();

// GOOD
let unit = pretty.split_whitespace().last()
    .ok_or_else(|| BtrfsError::ParseError("Invalid size format".into()))?;
```

**For Mutex locks:**
```rust
// BAD
*self.mount_result.lock().unwrap() = res.map_err(|e| e.to_string());

// GOOD
match self.mount_result.lock() {
    Ok(mut guard) => *guard = res.map_err(|e| e.to_string()),
    Err(poison) => {
        tracing::error!("Mutex poisoned: {}", poison);
        return Err(DiskError::Internal("Mutex poisoned".into()));
    }
}
```

**For static initialization (keep expects with context):**
```rust
// ACCEPTABLE (failure at startup is fatal)
.expect("Invalid log directive: cosmic_ext_disks=info")
```

**Acceptance Criteria:**
- [ ] No `unwrap()` in hot paths (called per-operation)
- [ ] No `unwrap()` in string parsing without validation
- [ ] Mutex poisoning handled or documented
- [ ] All production `expect()` have descriptive messages
- [ ] grep `unwrap\(` shows <20 hits in non-test code

---

### GAP-006: Unclear Client Ownership Model (Singleton vs Per-Operation)

**ID:** GAP-006  
**Type:** Architecture / API Design  
**Severity:** HIGH  
**Impact:** Confusion about lifecycle; potential resource leaks; inconsistent patterns

**Evidence:**

Different patterns used across the codebase:

**Pattern A: Per-operation creation**  
[disks-ui/src/models/ui_drive.rs:47-50](../disks-ui/src/models/ui_drive.rs#L47-L50):
```rust
pub async fn new(disk: DiskInfo) -> Result<Self, ClientError> {
    let client = DisksClient::new().await?;
    let partitions_client = PartitionsClient::new().await?;
    // Stored in struct
}
```

**Pattern B: Owned by UI model**  
Each `UiDrive` owns its own clients.

**Pattern C: Lazy initialization**  
[disks-ui/src/models/helpers.rs:93](../disks-ui/src/models/helpers.rs#L93):
```rust
let rt = tokio::runtime::Runtime::new().ok()?;
let client = rt.block_on(FilesystemsClient::new()).ok()?;
// Used once then dropped
```

**Questions Unanswered:**
1. Should there be **one** D-Bus connection per application?
2. Should clients be singletons in `AppModel`?
3. Should operations accept `&Client` as parameter?
4. What is the lifetime of a connection?

**zbus Behavior:**

According to zbus docs:
- `Connection::system()` may reuse existing connection (internal caching)
- Proxies are cheap to create (no separate connection per proxy)
- But creating many connections still wastes file descriptors

**Suggested Fix:**

**Recommended: Centralized client management**

```rust
// In AppModel
pub struct AppModel {
    core: Core,
    clients: Arc<ClientPool>,  // Shared across all operations
    // ...
}

pub struct ClientPool {
    disks: DisksClient,
    partitions: PartitionsClient,
    filesystems: FilesystemsClient,
    // etc.
}

impl ClientPool {
    pub async fn new() -> Result<Self, ClientError> {
        Ok(Self {
            disks: DisksClient::new().await?,
            partitions: PartitionsClient::new().await?,
            filesystems: FilesystemsClient::new().await?,
        })
    }
}

// UiDrive uses Arc<ClientPool> instead of owned clients
pub struct UiDrive {
    pub disk: DiskInfo,
    pub volumes: Vec<UiVolume>,
    pub partitions: Vec<PartitionInfo>,
    clients: Arc<ClientPool>,  // Shared reference
}
```

**Acceptance Criteria:**
- [ ] Document client lifecycle policy in `.copi/repo-rules.md`
- [ ] Consistent pattern used throughout disks-ui
- [ ] No redundant D-Bus connections
- [ ] Clear ownership semantics
- [ ] Tests can inject mock clients

---

### GAP-007: Storage-Service JSON Serialization Not Type-Safe

**ID:** GAP-007  
**Type:** Reliability / Type Safety  
**Severity:** MEDIUM  
**Impact:** Runtime serialization errors; schema mismatch between client/service; debugging difficulty

**Evidence:**

Every storage-service endpoint serializes responses to JSON strings:

[storage-service/src/disks.rs:90-94](../storage-service/src/disks.rs#L90-L94):
```rust
let json = serde_json::to_string(&disks)
    .map_err(|e| {
        tracing::error!("Failed to serialize disks: {e}");
        zbus::fdo::Error::Failed(format!("Serialization error: {e}"))
    })?;
Ok(json)
```

Then clients deserialize:

[disks-ui/src/client/disks.rs:89-91](../disks-ui/src/client/disks.rs#L89-L91):
```rust
let disks: Vec<DiskInfo> = serde_json::from_str(&json)
    .map_err(|e| ClientError::ParseError(format!("Failed to parse disk list: {}", e)))?;
```

**Problems:**
1. Type mismatch between service and client not caught at compile time
2. If service changes `DiskInfo` structure, clients silently fail at runtime
3. JSON string intermediate adds parsing overhead (small, but unnecessary)
4. Debugging serialization errors is harder (need to inspect JSON strings)

**Why Not Use zbus Native Serialization?**

zbus supports returning structured types directly via `zvariant`:
```rust
async fn list_disks(&self) -> zbus::fdo::Result<Vec<DiskInfo>> {
    // zbus auto-serializes Vec<DiskInfo> to D-Bus wire format
}
```

**Rationale in Current Design:**

Likely chosen because:
- Simpler to handle complex nested types
- JSON is easier to debug (`busctl` shows JSON strings)
- Avoids zvariant derive complexity

**Trade-off:**

Current approach is **pragmatic** but sacrifices type safety. This is **acceptable** if:
1. Service and client are always deployed together (same commit)
2. Integration tests validate serialization
3. Errors are logged clearly

**Suggested Fix (Optional Improvement):**

Add integration tests that validate serialization round-trips:

```rust
#[tokio::test]
async fn test_disk_info_serialization_roundtrip() {
    // Create DiskInfo
    let disk = DiskInfo { /* ... */ };
    
    // Service serializes
    let json = serde_json::to_string(&disk).unwrap();
    
    // Client deserializes
    let deserialized: DiskInfo = serde_json::from_str(&json).unwrap();
    
    // Validate fields match
    assert_eq!(disk.device, deserialized.device);
    // ...
}
```

**Acceptance Criteria:**
- [ ] Integration tests for all D-Bus interface types
- [ ] Serialization errors logged with type name and field info
- [ ] Document JSON-over-D-Bus decision in architecture.md
- [ ] (Optional) Prototype zvariant approach and benchmark performance

---

### GAP-008: No Validation in Partition Creation

**ID:** GAP-008  
**Type:** Security / Input Validation  
**Severity:** MEDIUM  
**Impact:** Invalid input can cause UDisks2 errors, confusing error messages, or undefined behavior

**Evidence:**

[storage-service/src/partitions.rs:182-257](../storage-service/src/partitions.rs#L182-L257):
```rust
async fn create_partition(
    &self,
    #[zbus(connection)] connection: &Connection,
    disk: &str,
    offset: u64,
    size: u64,
    type_id: &str,
) -> zbus::fdo::Result<String> {
    // Authorization check ✓
    // But no validation of:
    // - disk path format
    // - offset/size alignment
    // - size > 0
    // - offset + size <= disk capacity
    // - type_id format (GPT GUID vs DOS hex)
    
    let drives = disks_dbus::DriveModel::get_drives().await
        .map_err(|e| zbus::fdo::Error::Failed(format!("Failed to enumerate drives: {e}")))?;
```

**Missing Validations:**

1. **Device path format**: Should start with `/dev/`, exist, be a block device
2. **Offset alignment**: GPT requires 1MB alignment, DOS requires cylinder alignment
3. **Size constraints**: Must be > 0, <= available space
4. **Type ID format**: 
   - GPT: Must be valid UUID (e.g., `EBD0A0A2-B9E5-4433-87C0-68B6B72699C7`)
   - DOS: Must be 2-digit hex (e.g., `83`)
5. **Overlap detection**: New partition shouldn't overlap existing partitions

**Current Behavior:**

Invalid inputs are passed to UDisks2, which returns cryptic errors:
- `org.freedesktop.UDisks2.Error.Failed: GDBus.Error:org.freedesktop.UDisks2.Error.Failed: Failed to create partition`

User has no idea what went wrong.

**Suggested Fix:**

```rust
async fn create_partition(
    &self,
    #[zbus(connection)] connection: &Connection,
    disk: &str,
    offset: u64,
    size: u64,
    type_id: &str,
) -> zbus::fdo::Result<String> {
    // Authorization ✓
    check_polkit_auth(connection, "org.cosmic.ext.storage-service.partition-modify").await
        .map_err(|e| zbus::fdo::Error::from(e))?;
    
    // VALIDATION
    if !disk.starts_with("/dev/") {
        return Err(zbus::fdo::Error::InvalidArgs("Device path must start with /dev/".into()));
    }
    
    if size == 0 {
        return Err(zbus::fdo::Error::InvalidArgs("Partition size must be greater than zero".into()));
    }
    
    if offset % storage_models::GPT_ALIGNMENT_BYTES != 0 {
        return Err(zbus::fdo::Error::InvalidArgs(format!(
            "Offset must be aligned to {} bytes", storage_models::GPT_ALIGNMENT_BYTES
        )));
    }
    
    // Validate type_id format
    let table_type = get_partition_table_type(disk).await?;
    match table_type.as_str() {
        "gpt" => {
            if uuid::Uuid::parse_str(type_id).is_err() {
                return Err(zbus::fdo::Error::InvalidArgs("Invalid GPT type GUID".into()));
            }
        },
        "dos" => {
            if u8::from_str_radix(type_id, 16).is_err() {
                return Err(zbus::fdo::Error::InvalidArgs("Invalid DOS partition type (must be hex byte)".into()));
            }
        },
        _ => return Err(zbus::fdo::Error::InvalidArgs(format!("Unsupported partition table type: {}", table_type))),
    }
    
    // Validate size fits on disk
    let disk_info = get_disk_info_internal(disk).await?;
    if offset + size > disk_info.size {
        return Err(zbus::fdo::Error::InvalidArgs(format!(
            "Partition would exceed disk size ({})", 
            storage_models::bytes_to_pretty(disk_info.size)
        )));
    }
    
    // Proceed with creation...
}
```

**Acceptance Criteria:**
- [ ] All partition operations validate input parameters
- [ ] Clear error messages for invalid inputs
- [ ] Users see validation errors before UDisks2 operations
- [ ] Tests cover validation edge cases
- [ ] Validation errors returned as `InvalidArgs` FDO error type

---

### GAP-009: Conversions Module is Temporary Workaround

**ID:** GAP-009  
**Type:** Tech Debt / Architecture  
**Severity:** MEDIUM  
**Impact:** Extra code to maintain; performance overhead; violates intended architecture

**Evidence:**

[storage-service/src/conversions.rs:7-10](../storage-service/src/conversions.rs#L7-L10):
```rust
//! **NOTE:** This entire file is temporary and will be removed during Phase 3A refactoring.
//! Once disks-dbus returns storage-models types directly, no conversion will be needed.

pub fn drive_model_to_disk_info(drive: &disks_dbus::DriveModel) -> DiskInfo {
```

This module exists to convert `disks_dbus::DriveModel` → `storage_models::DiskInfo`.

**Why It Exists:**

During refactor, `storage-models` crate was created, but `disks-dbus` still returns old `DriveModel` types. The plan was to update `disks-dbus` in "Phase 3A" to return storage-models types directly.

**Current Status:**

Phase 3A **was completed** according to implementation logs, but this conversion module still exists. Likely:
1. Some disks-dbus paths return storage-models types (new API)
2. Other paths still return DriveModel (old API)
3. Conversions bridge the gap

**Impact:**
- Storage-service imports both `disks_dbus::DriveModel` and `storage_models::DiskInfo`
- Conversion overhead on every operation
- Code duplication (both types have similar fields)

**Suggested Fix:**

Complete Phase 3A migration:
1. Update all `disks-dbus` public APIs to return storage-models types
2. Remove `pub struct DriveModel` from disks-dbus (make it internal-only)
3. Delete `conversions.rs`
4. Update storage-service to only import storage-models

**Acceptance Criteria:**
- [ ] `conversions.rs` deleted
- [ ] No `disks_dbus::DriveModel` in storage-service
- [ ] All disks-dbus public functions return storage-models types
- [ ] Tests pass
- [ ] Benchmark: no performance regression

---

### GAP-010: TODO Comments Without Context (4 genuine TODOs)

**ID:** GAP-010  
**Type:** Tech Debt / Documentation  
**Severity:** LOW  
**Impact:** Unclear priorities; orphaned work items; noise in codebase

**Evidence:**

Grep found 100 matches for "TODO" but most are false positives (audit files, specs). Genuine TODOs in production code:

1. [disks-dbus/src/disks/volume.rs:651](../disks-dbus/src/disks/volume.rs#L651):
   ```rust
   parent_path: None, // TODO: Populate when building hierarchy
   ```
   **Status:** Critical bug (covered in GAP-004)

2. [disks-ui/src/ui/volumes/update/partition.rs:195](../disks-ui/src/ui/volumes/update/partition.rs#L195):
   ```rust
   // TODO: Implement flag checking methods on PartitionInfo
   ```
   **Context:** UI needs to check if partition is bootable/system/hidden

3. [disks-ui/src/ui/volumes/update/encryption.rs:126](../disks-ui/src/ui/volumes/update/encryption.rs#L126):
   ```rust
   // TODO: Implement get_encryption_options_settings via client
   ```
   **Context:** Edit encryption options UI not wired up

4. [disks-ui/src/ui/volumes/update/encryption.rs:301](../disks-ui/src/ui/volumes/update/encryption.rs#L301):
   ```rust
   // TODO: Implement take_ownership operation in storage-service
   ```
   **Context:** Change LUKS passphrase owner (advanced feature)

**Suggested Fix:**

Replace TODO comments with issue references:

```rust
// TODO: Populate parent_path  → // FIXME(#42): Populate parent_path during tree flattening
// TODO: Implement flag checking → // DEFERRED: Partition flags UI (see #43)
// TODO: Implement get_encryption_options → // BLOCKED: Requires storage-service LUKS settings API (#44)
```

**Acceptance Criteria:**
- [ ] All TODOs linked to GitHub issues or marked DEFERRED
- [ ] No orphaned TODO comments without context
- [ ] grep TODO shows only false positives or issue-linked TODOs

---

### GAP-011: Mutex Lock Panics Not Handled (30+ sites)

**ID:** GAP-011  
**Type:** Reliability / Error Handling  
**Severity:** MEDIUM  
**Impact:** Panic on mutex poisoning (prior panic while holding lock); unrecoverable state

**Evidence:**

[disks-dbus/src/disks/ops.rs](../disks-dbus/src/disks/ops.rs) has 30+ instances:

```rust
*self.create_result.lock().unwrap() = res.map_err(|e| e.to_string());
*self.mount_result.lock().unwrap() = res.map_err(|e| e.to_string());
// ... and 28 more
```

This pattern is in `MockDiskBackend` for testing. If a test panics while holding the lock, the mutex is "poisoned" and all subsequent `.lock().unwrap()` calls panic.

**Why It Matters:**

Tests should isolate failures. If test A panics and poisons a mutex, tests B, C, D will also panic with unhelpful messages:
```
thread 'test_mount' panicked at 'poisoned lock'
```

**Suggested Fix:**

Use `.lock().expect()` with context, or handle poisoning:

```rust
// Option A: Clear context
*self.mount_result.lock()
    .expect("MockDiskBackend mount_result mutex poisoned") = res;

// Option B: Handle poisoning
match self.mount_result.lock() {
    Ok(mut guard) => *guard = res,
    Err(poison) => {
        tracing::warn!("Mutex poisoned, clearing: {}", poison);
        *poison.into_inner() = res;
    }
}
```

**Acceptance Criteria:**
- [ ] All Mutex locks have `.expect()` with context or handle `PoisonError`
- [ ] Test failures don't cascade via poisoned mutexes
- [ ] Poisoning logged at warn level

---

### GAP-012: No Error Context in Client Error Conversions

**ID:** GAP-012  
**Type:** Observability / Error Handling  
**Severity:** LOW  
**Impact:** Hard to debug; errors lose context during conversion

**Evidence:**

[disks-ui/src/client/error.rs:26-55](../disks-ui/src/client/error.rs#L26-L55):
```rust
impl From<zbus::Error> for ClientError {
    fn from(err: zbus::Error) -> Self {
        match &err {
            zbus::Error::FDO(fdo_err) => {
                let error_str = fdo_err.to_string();
                
                if error_str.contains("AccessDenied") {
                    ClientError::PermissionDenied(error_str)
                } else if error_str.contains("ServiceUnknown") {
                    ClientError::ServiceNotAvailable
                } else {
                    // loses original error details
                    ClientError::MethodCall(error_str)
                }
            },
            _ => ClientError::Connection(err.to_string()),
        }
    }
}
```

**Problem:**

When converting `zbus::Error` → `ClientError`, only the error **message string** is preserved. Lost:
- zbus error variant details
- D-Bus error name (e.g., `org.freedesktop.UDisks2.Error.DeviceBusy`)
- Source error chain

This makes debugging harder when errors bubble up to UI.

**Suggested Fix:**

Preserve more context:

```rust
#[derive(Error, Debug, Clone)]
pub enum ClientError {
    #[error("D-Bus connection error: {message} (source: {source})")]
    Connection { message: String, source: String },
    
    #[error("D-Bus method call error: {message} (dbus_name: {dbus_name:?})")]
    MethodCall { message: String, dbus_name: Option<String> },
    
    // ...
}

impl From<zbus::Error> for ClientError {
    fn from(err: zbus::Error) -> Self {
        match &err {
            zbus::Error::FDO(fdo_err) => {
                let message = fdo_err.to_string();
                let dbus_name = Some(fdo_err.name().map(|n| n.to_string()));
                
                if message.contains("AccessDenied") {
                    ClientError::PermissionDenied { message, dbus_name }
                } else {
                    ClientError::MethodCall { message, dbus_name }
                }
            },
            _ => ClientError::Connection { 
                message: err.to_string(), 
                source: format!("{:?}", err) 
            },
        }
    }
}
```

**Acceptance Criteria:**
- [ ] ClientError includes D-Bus error names
- [ ] Error chains preserved where possible
- [ ] Logs show full error context
- [ ] Users see actionable error messages

---

### GAP-013: No Timeout Handling for Long-Running Operations

**ID:** GAP-013  
**Type:** Reliability / UX  
**Severity:** MEDIUM  
**Impact:** Hung UI on slow operations; no progress feedback; no cancellation

**Evidence:**

Operations like `format`, `create_partition`, `restore_image` can take minutes, but:
- No timeout configuration
- No progress reporting (except format has signals, but not hooked up in UI)
- No cancellation mechanism

Example: [disks-ui/src/client/filesystems.rs:141-150](../disks-ui/src/client/filesystems.rs#L141-L150):
```rust
pub async fn format(
    &self,
    device: &str,
    fs_type: &str,
    label: &str,
    options: Option<&str>,
) -> Result<(), ClientError> {
    let options_json = options.unwrap_or("{}");
    Ok(self.proxy.format(device, fs_type, label, options_json).await?)
}
```

This blocks until format completes (could be 10+ minutes for large ext4).

**Missing:**
1. Progress signals subscription
2. Timeout configuration
3. Cancellation API
4. User feedback during operation

**Suggested Fix:**

**Add progress tracking:**

```rust
pub struct FilesystemsClient {
    proxy: FilesystemsInterfaceProxy<'static>,
}

impl FilesystemsClient {
    pub async fn format_with_progress(
        &self,
        device: &str,
        fs_type: &str,
        label: &str,
        options: Option<&str>,
        progress_callback: impl Fn(f64) + Send + 'static,
    ) -> Result<(), ClientError> {
        // Subscribe to FormatProgress signal
        let mut stream = self.proxy.receive_format_progress().await?;
        
        // Spawn progress listener
        tokio::spawn(async move {
            while let Some(signal) = stream.next().await {
                if let Ok(args) = signal.args() {
                    progress_callback(args.progress);
                }
            }
        });
        
        // Start format operation
        self.proxy.format(device, fs_type, label, options.unwrap_or("{}")).await?;
        
        Ok(())
    }
}
```

**Add timeouts:**

```rust
use tokio::time::{timeout, Duration};

pub async fn format(&self, ...) -> Result<(), ClientError> {
    timeout(Duration::from_secs(600), self.proxy.format(...))
        .await
        .map_err(|_| ClientError::OperationFailed("Format timed out after 10 minutes".into()))??;
    Ok(())
}
```

**Acceptance Criteria:**
- [ ] Long-running operations have configurable timeouts
- [ ] Progress signals hooked up in UI
- [ ] Users see progress bar during format/restore
- [ ] Cancel button sends cancellation request
- [ ] Timeout errors are user-friendly

---

### GAP-014: Storage-Service Not Checking If Service Is Already Running

**ID:** GAP-014  
**Type:** Reliability / Deployment  
**Severity:** LOW  
**Impact:** Multiple instances fight for D-Bus name; confusing errors on startup

**Evidence:**

[storage-service/src/main.rs:54-77](../storage-service/src/main.rs#L54-L77):
```rust
let connection = ConnectionBuilder::system()?
    .name("org.cosmic.ext.StorageService")?
    // ... serve_at calls ...
    .build()
    .await?;
```

If another instance of `storage-service` is running, this will:
1. Fail with D-Bus name conflict
2. Error message: `Name org.cosmic.ext.StorageService already owned`

**But:**
- No user-friendly error message
- No PID check or lock file
- No graceful handling

**Suggested Fix:**

Add startup check:

```rust
#[tokio::main]
async fn main() -> Result<()> {
    // ... logging setup ...
    
    // Check if already running
    if is_service_already_running().await? {
        tracing::error!("Another instance of storage-service is already running");
        anyhow::bail!("Service already running. Stop the existing instance first.");
    }
    
    // ... continue startup ...
}

async fn is_service_already_running() -> Result<bool> {
    let conn = Connection::system().await?;
    match conn.request_name("org.cosmic.ext.StorageService").await {
        Ok(_) => {
            // We got the name, no other instance running
            Ok(false)
        }
        Err(e) if e.to_string().contains("already owned") => {
            Ok(true)
        }
        Err(e) => Err(e.into()),
    }
}
```

**Acceptance Criteria:**
- [ ] Clear error message if service already running
- [ ] Startup check done before initialization
- [ ] Logs show PID of existing instance (if possible)
- [ ] Systemd integration handles restarts cleanly

---

### GAP-015: No Integration Test Coverage for D-Bus Interfaces

**ID:** GAP-015  
**Type:** Testing / Quality Assurance  
**Severity:** MEDIUM  
**Impact:** Refactors break client-service contract; serialization bugs caught in production

**Evidence:**

No integration tests found for:
- D-Bus method call contracts (service ↔ client)
- JSON serialization round-trips
- Authorization checks
- Signal delivery

**Why It Matters:**

During the refactor:
1. `storage-service` was created
2. Clients were updated to call new service
3. No automated tests validate the integration

**Risks:**
- Service changes method signature → clients break
- Serialization format changes → silent data corruption
- Authorization policy changes → access denied errors in production

**Suggested Fix:**

Add integration test suite:

```rust
// tests/integration_test.rs

#[tokio::test]
async fn test_disks_list_disks() {
    // Start service (or use test harness)
    let service = start_test_service().await;
    
    // Create client
    let client = DisksClient::new().await.unwrap();
    
    // Call method
    let disks = client.list_disks().await.unwrap();
    
    // Validate response
    assert!(!disks.is_empty(), "Should return at least one disk");
    assert!(disks[0].device.starts_with("/dev/"));
}

#[tokio::test]
async fn test_partition_create_requires_auth() {
    let service = start_test_service().await;
    let client = PartitionsClient::new().await.unwrap();
    
    // Should fail without authorization
    let result = client.create_partition("/dev/sda", 0, 1024, "83").await;
    assert!(matches!(result, Err(ClientError::PermissionDenied(_))));
}
```

**Acceptance Criteria:**
- [ ] Integration tests for all D-Bus interfaces
- [ ] Tests cover happy path and error cases
- [ ] Authorization tests validate Polkit integration
- [ ] CI runs integration tests (may need mock UDisks2)
- [ ] Tests fail on serialization schema mismatches

---

### GAP-016: Unclear Atomic Update Strategy in UiDrive

**ID:** GAP-016  
**Type:** Architecture / Correctness  
**Severity:** MEDIUM  
**Impact:** Race conditions between refreshes; stale data in UI; performance overhead

**Evidence:**

[disks-ui/src/models/ui_drive.rs:97-125](../disks-ui/src/models/ui_drive.rs#L97-L125):
```rust
/// Atomically refresh a single volume by device path
/// 
/// This is much faster than refreshing the entire tree (3-5x speedup).
pub async fn refresh_volume(&mut self, device: &str) -> Result<bool, ClientError> {
    match self.client.get_volume_info(device).await {
        Ok(updated_vol_info) => {
            // Find the volume in the tree and update it
            for root in &mut self.volumes {
                if root.update_volume(device, &updated_vol_info) {
                    return Ok(true);
                }
            }
            Ok(false)
        }
        Err(e) => {
            tracing::warn!("Failed to atomically refresh volume {}: {}", device, e);
            Ok(false)
        }
    }
}
```

**Questions:**
1. What if a volume is deleted between reads? (update returns false, but UI still shows it)
2. What if multiple volumes change? (only one refreshed)
3. What if parent-child relationship changes? (tree structure stale)
4. When should we do full refresh vs atomic update?

**Race Condition Example:**

```
Time | Action                        | UiDrive State
-----|-------------------------------|----------------
T0   | User mounts /dev/sda1         | sda1 unmounted
T1   | UI calls refresh_volume()     | sda1 unmounted
T2   | (Concurrent) User also        | sda1 unmounted
     | creates /dev/sda2             |
T3   | get_volume_info() returns     | sda1 mounted
     | sda1 mounted                  |
T4   | update_volume() updates sda1  | sda1 mounted, 
     |                               | sda2 MISSING!
```

Now sda2 exists but isn't shown in UI until full refresh.

**Suggested Fix:**

Document refresh strategy:

```rust
/// # Refresh Strategy
/// 
/// - **Full refresh** (`refresh()`): Called on startup, after partition create/delete, or when device events fire
/// - **Atomic refresh** (`refresh_volume()`): Called after mount/unmount/format of existing volume
/// - **Not atomic-safe**: Partition creation/deletion (always triggers full refresh)
/// 
/// Atomic refresh is a performance optimization but NOT a correctness guarantee.
/// If atomic refresh fails or returns false, caller should schedule a full refresh.
```

Add consistency check:

```rust
pub async fn refresh_volume(&mut self, device: &str) -> Result<RefreshResult, ClientError> {
    match self.client.get_volume_info(device).await {
        Ok(updated_vol_info) => {
            for root in &mut self.volumes {
                if root.update_volume(device, &updated_vol_info) {
                    return Ok(RefreshResult::Updated);
                }
            }
            // Volume not found in tree - may have been deleted or tree is stale
            tracing::warn!("Volume {} not found in tree, scheduling full refresh", device);
            Ok(RefreshResult::NotFound)
        }
        Err(e) => {
            tracing::error!("Failed to refresh volume {}: {}", device, e);
            Ok(RefreshResult::Failed)
        }
    }
}

pub enum RefreshResult {
    Updated,
    NotFound,  // Caller should do full refresh
    Failed,
}
```

**Acceptance Criteria:**
- [ ] Refresh strategy documented in code
- [ ] Atomic refresh returns NotFound if volume missing
- [ ] UI schedules full refresh when atomic refresh fails
- [ ] Tests cover race conditions
- [ ] Concurrent operations don't cause stale data

---

## Quick Wins

These are small fixes with high value:

1. **Replace all `connection.as_ref().unwrap()` with proper errors** (GAP-002) → 2 hours
2. **Remove TODO comment in parent_path** (GAP-004) → 1 hour
3. **Add validation to partition create** (GAP-008) → 3 hours
4. **Add integration test scaffolding** (GAP-015) → 4 hours
5. **Document client lifecycle in repo-rules.md** (GAP-006) → 1 hour

---

## Open Questions

1. **What is the long-term plan for disks-dbus?**
   - Should it stay as a library, or be absorbed into storage-service?
   - Is it intended to be reused by other projects, or only by this one?

2. **Why was JSON-over-D-Bus chosen instead of native zvariant serialization?**
   - Performance concern?
   - Debugging convenience?
   - Complexity avoidance?

3. **Should there be caching in storage-service?**
   - Currently every D-Bus call enumerates all drives from scratch
   - Could cache `DriveModel::get_drives()` and invalidate on hotplug events

4. **Who owns error logging: service or client?**
   - Currently both log errors
   - Should service log and clients only display?

---

## Appendix: Search Commands Used

- `TODO|FIXME|HACK|XXX|NOT IMPLEMENTED|PLACEHOLDER` → 100 matches (mostly false positives in audit files)
- `unwrap\(|expect\(|panic!` → 100+ matches
- `unimplemented!|todo!` → 27 matches (mostly in specs/audits)
- `disks_dbus::|use disks_dbus` in storage-service → 50+ matches
- `Optional Connection|connection: Option` → 2 matches (VolumeNode, VolumeModel)
- `\.block_on\(|Runtime::new` → 5 matches (UiDrive, helpers)
- `\.clone\(\).*\.clone\(\)|Arc::new.*Arc::new` → 11 matches

---

## Revision History

| Date | Changes |
|------|---------|
| 2026-02-14 | Initial audit after storage-service refactor completion |

